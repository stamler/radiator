# (c) 2018 Dean Stamler
# Read logs generated by radiator.vbs, groom them to a
# consistent format (old versions) and upload to a charade API

from glob import iglob
from sys import exit
from helpers import InventoryLogFile
from os import path, getcwd, makedirs, scandir
from datetime import datetime
from shutil import move
from requests import post
import json
import logging

log = logging.getLogger(__name__)
log.setLevel(logging.DEBUG)
ch = logging.StreamHandler()
ch.setLevel(logging.INFO)
log.addHandler(ch)

config = {
    "api_endpoint": "http://localhost:9090/RawLogins",
    "search_path": path.join(getcwd(),'logs/'),
    "stage_path": path.join(getcwd(),'stage/'),
    "complete_path": path.join(getcwd(),'uploaded/')
}

def groom(search_path):
    try:
        # Check that the source path actually exists
        if(path.isdir(config['search_path']) is not True):
            msg = "{} is not a directory".format(config['search_path'])
            log.error(msg)
            raise NotADirectoryError(msg, config['search_path'])

        # Create staging and complete dirs. Throw an exception if stage
        # exists because it may not be empty (incomplete previous run)
        resume_mode = False
        makedirs(config['complete_path'], exist_ok=True)
        makedirs(config['stage_path'])

    except NotADirectoryError as e:
        exit(1)

    except FileExistsError as e:
        # If stage_dir is not empty, enter resume mode
        if next(scandir(config['stage_path']), None) is not None:
            log.warning("stage_path is not empty. Resuming previous session...")
            resume_mode = True

    # Move log files in search_path to stage_path unless we're resuming.
    # Add timestamp to name.
    if(resume_mode is False):
        for filename in iglob(path.join(search_path,'**/*.log'), recursive=True):
            timestamp = datetime.now().strftime("%Y%m%d%H%M%S-%f")
            dest = (config['stage_path'] +
                    path.basename(filename) + "_" + timestamp + ".log")
            move(filename, dest)

    linecount = 0

    for filename in iglob(path.join(config['stage_path'],'*.log')):
        try:
            inventory_log_file = InventoryLogFile(filename)
        except ValueError as e:
            log.error(e)
            log.error("Skipping file {}".format(filename))
            continue

        lc = len(inventory_log_file.lines)

        # We can now output the lines in a groomed format
        linecount += lc

        headers = {'Content-Type': 'application/json',
                                        'Accept':'application/json'}
        data = inventory_log_file.to_json()

        log.info("Sending {} items.".format(lc))

        r = post(config['api_endpoint'], data=data, headers=headers)

        # Possible responses:
        #   201: Created a single object
        #   201: Created multiple objects at once
        #   207: Created multiple objects (under max_multi, see charade)

        data = json.loads(r.text)['data']

        if r.status_code == 201:
            rowcount = data['rowcount']
            if lc == rowcount:
                log.info("  201 Saved {} items.".format(rowcount))
        elif r.status_code == 207:
            rowcount = 0
            for i in data:
                if ( i['client_id'] in inventory_log_file.client_ids and
                        i['status'] == '201 Created' and
                        i['body']['rowcount'] == 1 ):
                    log.debug("  returned client_id {} was sent".format(
                                                            i['client_id']))
                    rowcount += 1
            if lc == rowcount:
                log.info("  207 Saved and matched {} items.".format(rowcount))

        else:
            log.error("FAILURE: A non-successful HTTP status was returned")

        inventory_log_file.close_file()

        # Move the file from stage_path to complete_path
        move(filename, config['complete_path'])

    log.info('\ngroomed {} lines'.format(linecount))

groom(config['search_path'])
